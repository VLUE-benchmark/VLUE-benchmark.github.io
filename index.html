<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> 
<![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8" lang="en"> 
<![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9" lang="en"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
    <head>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-153119114-2"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());
		  gtag('config', 'UA-153119114-2');
		</script>

        <title>VLUE | ByteDance AI LAB </title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0"> 
        <meta name="author" content="VLUE">
        <meta charset="UTF-8">

        <!-- CSS Bootstrap & Custom -->
        <link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
        <link href="css/font-awesome.min.css" rel="stylesheet" media="screen">
		<link href="css/animate.css" rel="stylesheet" media="screen">
		
		<meta property="og:site_name" content="VLUE | ByteDance AI Lab">
		<meta property="og:title" content="VLUE | A Multi-Task Multi-Dimension Benchmark for Evaluating Vision-Language Pre-training">
		<meta name=description content="Authors: Wangchunshu Zhou, Yan Zeng, Shizhe Diao, Xinsong Zhang">
		<meta name=keywords content="VLUE, Wangchunshu Zhou, Yan Zeng, Shizhe Diao, Xinsong Zhang, Natural Language Processing, Computational Linguistics, ByteDance, ByteDance AI Lab, HKUST, Machine Learning, StatNLP, NLP, ML, Cross-Cultural">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<script language="javascript">
				var ie4 = false; if(document.all) { ie4 = true; }
			function getObject(id) { if (ie4) { return document.all[id]; } else { return document.getElementById(id); } }
			function toggle(link, divId) { var lText = link.innerHTML; var d = getObject(divId);
			 if (lText == '[show more]') { link.innerHTML = '[hide more]'; d.style.display = 'block'; }
			 else { link.innerHTML = '[show more]'; d.style.display = 'none'; } }
			   </script>
<script>
$(document).ready(function () {
$('#dtBasicExample').DataTable();
$('.dataTables_length').addClass('bs-select');
});
</script>

<link href="https://unpkg.com/bootstrap-table@1.15.5/dist/bootstrap-table.min.css" rel="stylesheet">

<script src="https://unpkg.com/bootstrap-table@1.15.5/dist/bootstrap-table.min.js"></script>

<style type="text/css">
table.dataTable thead .sorting:after,
table.dataTable thead .sorting:before,
table.dataTable thead .sorting_asc:after,
table.dataTable thead .sorting_asc:before,
table.dataTable thead .sorting_asc_disabled:after,
table.dataTable thead .sorting_asc_disabled:before,
table.dataTable thead .sorting_desc:after,
table.dataTable thead .sorting_desc:before,
table.dataTable thead .sorting_desc_disabled:after,
table.dataTable thead .sorting_desc_disabled:before {
bottom: .5em;
}
</style>
		<link href="style.css" rel="stylesheet" media="screen">
		<!-- MDBootstrap Datatables  -->
		<link href="css/addons/datatables.min.css" rel="stylesheet">
		<!-- MDBootstrap Datatables  -->
		<script type="text/javascript" src="js/addons/datatables.min.js"></script>
        <!-- Favicons -->
        <link rel="apple-touch-icon-precomposed" sizes="144x144" href="images/ico/apple-touch-icon-144-precomposed.png">
        <link rel="apple-touch-icon-precomposed" sizes="114x114" href="images/ico/apple-touch-icon-114-precomposed.png">
        <link rel="apple-touch-icon-precomposed" sizes="72x72" href="images/ico/apple-touch-icon-72-precomposed.png">
        <link rel="apple-touch-icon-precomposed" href="images/ico/apple-touch-icon-57-precomposed.png">
        <link rel="shortcut icon" href="images/ico/favicon.ico">
    
        <!-- JavaScripts -->
        <script src="js/jquery-1.10.2.min.js"></script>
        <script src="js/min/modernizr.min.js"></script>
        <!--[if lt IE 8]>
	    <div style=' clear: both; text-align:center; position: relative;'>
            <a href="http://www.microsoft.com/windows/internet-explorer/default.aspx?ocid=ie6_countdown_bannercode"><img src="http://storage.ie6countdown.com/assets/100/images/banners/warning_bar_0000_us.jpg" border="0" alt="" /></a>
        </div>
		<![endif]-->
		<link href="js/bootstrap-table.min.css" rel="stylesheet">

<script src="js/bootstrap-table.min.js"></script>
		
    </head>
    <body>
	
	
		<div class="responsive-menu visible-sm visible-xs">
			<a href="#" class="toggle-menu"><i class="fa fa-bars"></i></a>
			<div class="menu-open">
				<nav>
						<ul class="sf-menu">
								<li ><a href="index.html">Introduction</a></li> 
								<!-- <li ><a href="https://huggingface.co/nlp/viewer/?dataset=common_gen " target="_blank">Data Explorer</a></li> -->
								<li ><a href="leaderboard.html">Leaderboard</a></li>
								<li ><a href="analysis.html">Analysis</a></li>
								<li ><a href="#misc">Misc.</a></li>
							</ul>
				</nav> 
			</div> <!-- /.menu-open -->
		</div> <!-- /.responsive-menu -->

		<header class="site-header">
			
			<div class="container">
				<div class="main-header">
					<div class="row">
						
						<div class="col-md-5 col-sm-4" id="top">
							<h1 style="color: white"><b>VLUE</b></em></h1>
							<span style="color:aliceblue; font-size: 15pt; font-family: Roboto, Helvetica, Arial, Heveltica Neue, sans-serif"> VLUE: A Multi-Task Multi-Dimension Benchmark <br> for Evaluating Vision-Language Pre-training</span> <br>

							<span style="color:aliceblue; font-size: 12pt; font-family: Roboto, Helvetica, Arial, Heveltica Neue, sans-serif">
								<br> 
								<a target="_blank" href="https://michaelzhouwang.github.io/">Wangchunshu Zhou</a>, 
								Yan Zeng,
								<a target="_blank" href="https://shizhediao.github.io/">Shizhe Diao</a>, 
								Xinsong Zhang
								<br>
							</span>
							<br>
						</div> 
					</div> <!-- /.row -->
				</div> <!-- /.main-header -->
			</div> <!-- /.container -->
			<div class="menu-wrapper visible-md visible-lg">
				<div class="container">
					<div class="inner-menu">
						<div class="row">
							<div class="col-md-12 main-menu">
								<nav>
									<ul class="sf-menu"> 
								<li ><a href="index.html">Introduction</a></li> 
								<!-- <li ><a href="https://huggingface.co/nlp/viewer/?dataset=common_gen " target="_blank">Data Explorer</a></li> -->
								<li ><a href="leaderboard.html">Leaderboard</a></li>
								<li ><a href="analysis.html">Analysis</a></li>
								<li ><a href="#misc">Misc.</a></li>
										<!-- <li ><a href="#blog">Blogs</a></li>  -->
										<!-- <li><a href="#contact">Contact</a></li> -->
									</ul>
								</nav>
							</div> <!-- /.main-menu --> 
						</div> <!-- /.row -->
					</div> <!-- /.inner-menu -->
				</div> <!-- /.container -->
			</div> <!-- /.menu-wrapper -->
		</header> <!-- /.site-header -->


		<div class="container">
			
			<div class="top-content">
				
 

			<div class="row">
					<div class="col-md-12">
						<div class="box-content">
							<div class="row">
								<div class="col-md-12" id="about">
									<!-- <h2 class="widget-title">(Bill) Yuchen Lin</h2> -->
								<h4 class="widget-title"><span><a href="#about" ><b><em>Introduction</em></b></a></span></h4>
								<div class="col-md-5"> 
									<p style="font-size:16px">
										Recent advances in vision-language pre-training (VLP) have demonstrated impressive performance in a range of vision-language (VL) tasks. However, there exist several challenges for measuring the community’s progress in building general multi-modal intelligence. First, most of the downstream VL datasets are annotated using raw images that are already seen during pre-training, which may result in an overestimation of current VLP models’ generalization ability. Second, recent VLP work mainly focuses on absolute performance but overlooks the efficiency-performance trade-off, which is also an important indicator for measuring progress. To this end, we introduce the Vision-Language Understanding Evaluation (VLUE) benchmark, a multi-task multi-dimension benchmark for evaluating the generalization capabilities and the efficiency-performance trade-off (“Pareto SOTA”) of VLP models. We demonstrate that there is a sizable generalization gap for all VLP models when testing on out-of-distribution test sets annotated on images from a more diverse distribution that spreads across cultures. Moreover, we find that measuring the efficiency-performance trade-off of VLP models leads to complementary insights for several design choices of VLP. We release the VLUE benchmark to promote research on building vision-language models that generalize well to images unseen during pre-training and are prac- tical in terms of efficiency-performance trade-off.
<!-- <br> <br>
CommonGen is challenging because it inherently requires 1) relational reasoning using background commonsense knowledge, and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowd-sourcing from AMT and existing caption corpora, consists of 30k concept-sets and 50k sentences in total.
									</p> -->


									<p style="text-align: left;"> 
											 	<font color="blue"><b>Links:</b></font> &nbsp; 
									
									<a style="color:blue;" target="_blank" href="https://arxiv.org/abs/1911.03705">[Paper]</a> &nbsp;
									<a style="color:blue;" target="_blank" href="leaderboard.html">[Leaderboard]</a> &nbsp;
									<a style="color:blue;" target="_blank" href="https://forms.gle/k181RNK8jPNpKvHD8">[Data]</a> &nbsp;
									<!-- <a target="_blank" href="https://huggingface.co/nlp/viewer/?dataset=common_gen ">[Huggingface Viewer]</a> &nbsp; -->
									<a style="color:blue;" target="_blank" href="https://github.com/INK-USC/CommonGen">[Github]</a> &nbsp;  
									<!-- <a style="color:blue;" target="_blank" href="https://ailab.bytedance.com/">[ByteDance AI Lab]</a> &nbsp;  -->
									<br>
						 
									</p>  
									
								</div>
								<div class="col-md-7"  align="center"> <img width="95%" src="intro.png" alt="VLUE">
								 </div>
									</div>

								<div  class="col-md-12">
								</div>
								</div>
								<img src="./images/new.jpeg" width="40" height="20"> 
								<a style="color:#9c0004;"> We are looking for interns/FTEs at <a href="https://ailab.bytedance.com/"> ByteDance AI-LAB </a> <a style="color:#9c0004;"> (in Beijing / Shanghai)! If you are interested in working with us on vision language models, please send your resume to <a href = "mailto: zhangxinsong.0320@bytedance.com">zhangxinsong.0320@bytedance.com</a>

								</div>
								
								</div>
								 
									</div> <!-- /.col-md-4 -->
								</div>
								


			<div class="row">
			
				<div class="col-md-12">
				<div class="box-content">
						<div class="row">
							<div class="col-md-12" id="exp">
								<h4 class="widget-title"><span><b><a><em>Key Challenges</em></a></b></span></h4>

								<div class="col-md-12" id="ser"  align="center">
										<img width="95%" src="challenge.png">
										<br><br>

										<div   align="left">
										<b>Why is this problem hard?</b><br>

										To address these problems and promote research on truly generalizable and practical VLP, we introduce the <b>V</b>ision-<b>L</b>anguage <b>U</b>nderstanding <b>E</b>valuation (<b>VLUE</b>) benchmark. 
										VLUE is the first multi-task benchmark focusing on vision-language understanding that covers a set of fundamental VL tasks including image-text retrieval, visual question answering, visual reasoning, and visual grounding, and maintains a leaderboard tracking the performance of representative studies and new methods on VLP. 
										More importantly, VLUE includes a newly annotated private out-of-distribution (OOD) test set for each representative VL task. In contrast to standard datasets for these tasks that are annotated on COCO/VG images, our private OOD test sets are annotated on images from the MaRVL (Liu et al., 2021a) dataset where images are manually collected across cultures by native speakers from different countries. 
										This ensures that the image distribution in our OOD test sets differs from that of COCO/VG images. Moreover, we carefully control the annotation protocol for our OOD test sets to be identical to the original in-domain datasets. As such, the label distribution in our OOD test sets is roughly the same as the original test set but the image distribution differs. This enables us to better measure the true generalization and transferability of VLP models. In addition, we also encourage researchers to measure and compare the efficiency-performance trade-off when reporting new studies in the field of VLP. To facilitate that, we measure the efficiency-performance trade-off of representative VLP models in \benchmark to track a Pareto SOTA landscape for VLP research. 
										In general, in contrast to conventional benchmarks that only capture the single performance metric, VLUE is a multi-dimension benchmark that takes multiple dimensions including performance, generalization ability, and efficiency into account. Hopefully, this will promote research on VLP models that are environmentally friendly and practical for real-world applications.
										
										We evaluate a range of representative VLP models on \benchmark to facilitate future research and analyze their generalization ability and efficiency-performance trade-off with respect to several key design choices. We find that there is a sizable generalization gap for all VLP models when evaluating on new examples annotated with images from in-the-wild distribution. Also, compared to focusing on a single dimension (i.e., absolute performance), measuring the generalization ability of different models can lead to complementary and even controversial conclusions. We also find that models with similar performance may result in completely different positions in the Pareto front measuring the efficiency-performance trade-off of VLP models, which also demonstrates the necessity of a multi-dimension benchmark for evaluating VLP models. 

</div>
									</div>
							</div>
						</div>
						
					</div> <!-- /.box-content -->


						
				</div> <!-- /.col-md-8 -->
				
				

			</div> <!-- /.row -->

<div class="box-content">
								<div class="row">
									<div class="col-md-12" id="misc">
										<h4 class="widget-title"><span><b>Misc.</b></span></h4>
		<div style="font-size:15px"> 
			 <h4><b>Citation</b></h4>

<pre><code>@inproceedings{zhou-etal-2022-vlue,
    title = "VLUE: A Multi-Task Multi-Dimension Benchmark for Evaluating Vision-Language Pre-training",
    author = "Zhou, Wangchunshu and
      Zeng, Yan  and
      Diao, Shizhe  and
      Zhang, Xinsong",
    month = jan,
    year = "2022",
    address = "Online",
}
</code></pre>
		</div>
	</div>
</div>

</div>

		</div> <!-- /.container -->


		 
		<footer class="site-footer">
			<div class="main-footer">

				<div class="container"> 
					<p class="small-text">Copyright 2022 &copy;.  ByteDance AI Lab
								</p> 
					<div class="copyright">
						<div class="row">
							<div class="col-md-6 col-sm-6">
								
							</div> <!-- /.col-md-6 -->

						</div> <!-- /.row -->
					</div> <!-- /.copyright -->
				</div> <!-- /.container -->
			</div> <!-- /.main-footer -->
		</footer> <!-- /.site-footer -->

		<a href="#about" id="top-link" class="fa fa-angle-up"></a>
	
        <!-- JavaScripts -->
        <script src="js/bootstrap.min.js"></script>
        <script src="js/min/plugins.min.js"></script>
        <script src="js/min/custom.min.js"></script>
		
		<!-- Default Statcounter code for vlue
https://vlue-benchmark.github.io/VLUE-website/ -->
<script type="text/javascript">
	var sc_project=12720372; 
	var sc_invisible=1; 
	var sc_security="58f9433d"; 
	</script>
	<script type="text/javascript"
	src="https://www.statcounter.com/counter/counter.js"
	async></script>
	<noscript><div class="statcounter"><a title="Web Analytics
	Made Easy - Statcounter" href="https://statcounter.com/"
	target="_blank"><img class="statcounter"
	src="https://c.statcounter.com/12720372/0/58f9433d/1/"
	alt="Web Analytics Made Easy - Statcounter"
	referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
	<!-- End of Statcounter Code -->
	
    </body>
</html>
